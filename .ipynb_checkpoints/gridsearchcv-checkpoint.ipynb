{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo de GridSearchCV do [blog](gusrabbit.com) do Gustavo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro importamos o que vamos usar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos carregar os dados do dataset de boston do scikit. Esse dataset tem características e preços de casas em boston, vamos usar para fazer uma regressão! Se você apertar **tab** depois do ponto você vai conseguir ver outros datasets que já vem no scikit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que nem da [outra vez](https://gusrabbit.com/code/cross_validate/), a gente separa a variável objetivo das features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing.data\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos pegar um modelo basicão, o DummyRegressor. Ele é um regressor cuja previsão é aleatória. É legal usar esses modelos aleatórios como baseline para comparar a qualidade dos nossos modelos. No mínino tem que ser melhor que isso. Vamos instanciar ele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = DummyRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acordo com a [documentação](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ele precisa dos parâmetros a seguir:\n",
    "\n",
    "GridSearchCV(estimator, **param_grid**, **scoring=None**, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2\\*n_jobs’, error_score=’raise’, return_train_score=’warn’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas primeiro! Vamos definir as métricas que queremos medir, lembrando que todas estão no [link do amor](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter). Vamos definir uma lista com todas as de regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas = ['explained_variance', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que o tchan do Grid Search é exatamente ele fazer os cross validation de vários modelos com hiperparâmetros diferentes de uma vez. Então vamos definir o **param_grid** que vai definir quais modelos com quais parâmetros o nosso Grid Search vai rodar.\n",
    "\n",
    "Se você olhar na [documentação](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) ela diz que pode ser um dicionário ou uma lista de dicionários, qual a ideia?\n",
    "\n",
    "A vibe é definir um dicionário com os parâmetros do nosso estimador. Vamos usar o método get_params pra ver o nome certinho dos parâmetros do nosso DummyRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas são as chaves do nosso dict, os valores são aqueles que queremos que o Grid Search rode. Criamos um dict assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper = {'strategy':['mean', 'median', 'quantile', 'constant'],\n",
    "         'quantile':[.75],\n",
    "         'constant':[300000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, o nosso Grid Search vai rodar quatro modelos, cada um com uma das estratégias. Isso acontece, pois os parâmetros quantile e constant só são usados naquelas duas estratégias. Vamos definir o Grid Search com tudo que a gente montou até agora e com *verbose=100* pra ele ir falando o que ele ta fazendo passo a passo. Vamos mandar ele fazer o refit no *neg_mean_squared_error*, mas só porque esse é a minha métricas preferida. Isso significa que ele vai escolher o modelo com o menor erro quadrado médio. E por último, a gente pede pra n retornar os scores de treinamento porque eles são meio inúteis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_primeiro_grid = GridSearchCV(baseline, param_grid=hyper, scoring=metricas, verbose=100, refit='neg_mean_squared_error', return_train_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por padrão, essa função vai separar o dataset em 3 partes e vai devolver a métrica média de cada uma dessas 3. No futuro eu vou postar sobre Kfold e como podemos separar os dados em várias partes para fazer o cross validation como aqui. Tudo isso é controlado pelo parâmetro *cv*. Agora vamos mandar o Grid Search rodar nos dados de boston!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_primeiro_grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos dar uma olhada nos resultados, o melhor estimador foi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_primeiro_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E o melhor score foi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_primeiro_grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que diabéisso? Ok, como a gente colocou *neg_mean_squared_error* no refite ele está devolvendo o melhor score dessa métrica. Essa métrica da a média do quadrado dos erros, só q negativa. Por que negativa? Acho que é pra ficar mais fácil de minimizar, aí eles adotam o padrão de colocar as métricas tudo negativa. O erro é a diferença entre o que o modelo preveu e o valor verdadeiro da casa. Aqui como a melhor estratégia foi a mediana, a previsão do modelo foi a mediana dos valores de treinamento pra toda casa nova. A gente pega a diferença do prebisto pelo verdadeiro, eleva ao quadrado (normal em estatística pra evitar que erros negativos e positivos se anulem, e é mais fácil de derivar do que usar módeulo).\n",
    "\n",
    "Então pra ter uma ideia se isso ta bom ou ruim basta tirar a raiz disso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(meu_primeiro_grid.best_score_*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em média nosso modelo errou o preço por 10 mil dólares. O Grid Search retorna um dicionário com todos os resultados, eu gosto de transformar ele num dataframe pra ficar mais fácil de ver (antes vou usar uma manhã do pandas pra que ele mostre todas as colunas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns',200)\n",
    "pd.DataFrame(meu_primeiro_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver se uma regressão linear simples fica melhor? Bora instanciar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os únicos parâmetros interessantes para a gente são o *fit_intercept* e o *normalize*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_params = {'fit_intercept':[True, False],\n",
    "              'normalize':[True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos definir o Grid Search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_segundo_grid = GridSearchCV(ols, param_grid=ols_params, scoring=metricas, verbose=100, refit='neg_mean_squared_error', return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_segundo_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_segundo_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu_segundo_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(meu_segundo_grid.best_score_*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regressão ta dando um erro de 12!!! Pior do que sempre chutar a mediana! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
